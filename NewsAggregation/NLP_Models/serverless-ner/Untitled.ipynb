{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9f88d1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parthshah/Documents/Northeastern/Spring2022/BigDataAnalytics/Assignment3/.venv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading: 100%|███████████████████████████| 850M/850M [00:17<00:00, 50.6MB/s]\n",
      "Downloading: 100%|███████████████████████████| 773k/773k [00:00<00:00, 5.80MB/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "# initialize the model architecture and weights\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
    "# initialize the model tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d264471",
   "metadata": {},
   "outputs": [],
   "source": [
    "article = \"\"\"\n",
    "A frontal boundary that had been stalled over the mid-Atlantic had lifted north of the region by the morning of \n",
    "June 29. Later that day and into the evening, the front once again approached, this time as a strong cold front,\n",
    "as low pressure tracked through New England and began to intensify offshore in the Gulf of Maine. \n",
    "The combination of strong frontal forcing and a warm, unstable environment ahead of the front led to \n",
    "widespread severe thunderstorms developing. Numerous reports of damaging wind, as well as some hail, \n",
    "were received in association with these storms.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91919ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode the text into tensor of integers using the appropriate tokenizer\n",
    "inputs = tokenizer.encode(\"summarize: \" + article, return_tensors=\"pt\", max_length=512, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "13b05a34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[21603,    10,    71,   851,   138, 20430,    24,   141,   118,     3,\n",
       "          8407,  1361,   147,     8,  2076,    18,   188,    17,  1618,  1225,\n",
       "           141, 19464,  3457,    13,     8,  1719,    57,     8,  1379,    13,\n",
       "          1515,  2838,     5, 10511,    24,   239,    11,   139,     8,  2272,\n",
       "             6,     8,   851,   728,   541, 15319,     6,    48,    97,    38,\n",
       "             3,     9,  1101,  2107,   851,     6,    38,   731,  1666, 22679,\n",
       "           190,   368,  2789,    11,  1553,    12,  9608,  4921, 16667,    16,\n",
       "             8, 13566,    13, 13905,     5,    37,  2711,    13,  1101,   851,\n",
       "           138, 19060,    11,     3,     9,  1978,     6, 27644,  1164,  2177,\n",
       "            13,     8,   851,  2237,    12, 14047,  5274, 31250,     7,  2421,\n",
       "             5, 25638,   302,  2279,    13, 18488,  2943,     6,    38,   168,\n",
       "            38,   128, 22690,     6,   130,  1204,    16,  6028,    28,   175,\n",
       "          5536,     7,     5,     1]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d3d6744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,     3,     9,   851,    24,   141,   118,     3,  8407,  1361,\n",
      "           147,     8,  2076,    18,   188,    17,  1618,  1225, 19464,  3457,\n",
      "            13,     8,  1719,     3,     5,   865,    24,   239,    11,   139,\n",
      "             8,  2272,     6,     8,   851,   728,   541, 15319,     3,     5,\n",
      "           731,  1666, 22679,   190,   126,  2789,    11,  1553,    12,  9608,\n",
      "          4921, 16667,    16,     8, 13566,    13, 13905,     3,     5,     1]])\n",
      "<pad> a front that had been stalled over the mid-Atlantic lifted north of the region. later that day and into the evening, the front once again approached. low pressure tracked through new England and began to intensify offshore in the Gulf of Maine.</s>\n"
     ]
    }
   ],
   "source": [
    "# generate the summarization output\n",
    "outputs = model.generate(\n",
    "    inputs, \n",
    "    max_length=150, \n",
    "    min_length=40, \n",
    "    length_penalty=2.0, \n",
    "    num_beams=4, \n",
    "    early_stopping=True)\n",
    "# just for debugging\n",
    "print(outputs)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b62c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, AutoConfig\n",
    "\n",
    "def encode(tokenizer, question, context):\n",
    "    \"\"\"encodes the question and context with a given tokenizer\"\"\"\n",
    "    encoded = tokenizer.encode_plus(question, context)\n",
    "    return encoded[\"input_ids\"], encoded[\"attention_mask\"]\n",
    "\n",
    "def decode(tokenizer, token):\n",
    "    \"\"\"decodes the tokens to the answer with a given tokenizer\"\"\"\n",
    "    answer_tokens = tokenizer.convert_ids_to_tokens(\n",
    "        token, skip_special_tokens=True)\n",
    "    return tokenizer.convert_tokens_to_string(answer_tokens)\n",
    "\n",
    "def serverless_pipeline(model_path='./model'):\n",
    "    \"\"\"Initializes the model and tokenzier and returns a predict function that ca be used as pipeline\"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModelForQuestionAnswering.from_pretrained(model_path)\n",
    "    def predict(question, context):\n",
    "        \"\"\"predicts the answer on an given question and context. Uses encode and decode method from above\"\"\"\n",
    "        input_ids, attention_mask = encode(tokenizer,question, context)\n",
    "        a =  model(torch.tensor(\n",
    "            [input_ids]), attention_mask=torch.tensor([attention_mask]))\n",
    "        start_scores = a['start_logits']\n",
    "        end_scores = a['end_logits']    \n",
    "        print(start_scores)\n",
    "        ans_tokens = input_ids[torch.argmax(\n",
    "            start_scores): torch.argmax(end_scores)+1]\n",
    "        answer = decode(tokenizer,ans_tokens)\n",
    "        return answer\n",
    "    return predict\n",
    "\n",
    "# initializes the pipeline\n",
    "question_answering_pipeline = serverless_pipeline()\n",
    "\n",
    "def handler(event, context):\n",
    "    try:\n",
    "        # loads the incoming event into a dictonary\n",
    "        body = json.loads(event['body'])\n",
    "        # uses the pipeline to predict the answer\n",
    "        answer = question_answering_pipeline(question=body['question'], context=body['context'])\n",
    "        return {\n",
    "            \"statusCode\": 200,\n",
    "            \"headers\": {\n",
    "                'Content-Type': 'application/json',\n",
    "                'Access-Control-Allow-Origin': '*',\n",
    "                \"Access-Control-Allow-Credentials\": True\n",
    "\n",
    "            },\n",
    "            \"body\": json.dumps({'answer': answer})\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(repr(e))\n",
    "        return {\n",
    "            \"statusCode\": 500,\n",
    "            \"headers\": {\n",
    "                'Content-Type': 'application/json',\n",
    "                'Access-Control-Allow-Origin': '*',\n",
    "                \"Access-Control-Allow-Credentials\": True\n",
    "            },\n",
    "            \"body\": json.dumps({\"error\": repr(e)})\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d94892b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parthshah/Documents/Northeastern/Spring2022/BigDataAnalytics/Assignment3/.venv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from transformers import pipeline\n",
    "\n",
    "ner = pipeline(\"ner\", model='./model', tokenizer='./model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bd22f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "event = {\"body\":\"{\\\"context\\\":\\\"We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).\\\"}\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d86829a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'B-LOC', 'score': 0.9885201, 'index': 69, 'word': 'Plains', 'start': 346, 'end': 352}, {'entity': 'B-LOC', 'score': 0.9972579, 'index': 72, 'word': 'Great', 'start': 360, 'end': 365}, {'entity': 'I-LOC', 'score': 0.9981468, 'index': 73, 'word': 'Lakes', 'start': 366, 'end': 371}, {'entity': 'B-LOC', 'score': 0.9952852, 'index': 87, 'word': 'Cape', 'start': 428, 'end': 432}, {'entity': 'I-LOC', 'score': 0.99629253, 'index': 88, 'word': 'G', 'start': 433, 'end': 434}, {'entity': 'I-LOC', 'score': 0.9961098, 'index': 89, 'word': '##ira', 'start': 434, 'end': 437}, {'entity': 'I-LOC', 'score': 0.9663395, 'index': 90, 'word': '##rde', 'start': 437, 'end': 440}, {'entity': 'I-LOC', 'score': 0.9272877, 'index': 91, 'word': '##au', 'start': 440, 'end': 442}, {'entity': 'B-LOC', 'score': 0.9960742, 'index': 98, 'word': 'Perry', 'start': 466, 'end': 471}, {'entity': 'I-LOC', 'score': 0.9977574, 'index': 99, 'word': '##ville', 'start': 471, 'end': 476}, {'entity': 'B-LOC', 'score': 0.9915392, 'index': 106, 'word': 'Pop', 'start': 500, 'end': 503}, {'entity': 'I-LOC', 'score': 0.94390976, 'index': 107, 'word': '##lar', 'start': 503, 'end': 506}, {'entity': 'I-LOC', 'score': 0.99718875, 'index': 108, 'word': 'Bluff', 'start': 507, 'end': 512}, {'entity': 'B-LOC', 'score': 0.9923455, 'index': 116, 'word': 'Si', 'start': 540, 'end': 542}, {'entity': 'I-LOC', 'score': 0.9254696, 'index': 117, 'word': '##kes', 'start': 542, 'end': 545}, {'entity': 'I-LOC', 'score': 0.7249604, 'index': 118, 'word': '##ton', 'start': 545, 'end': 548}]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import ast\n",
    "\n",
    "event = {\"context\":\"D2 drought conditions developed across portions of Geneva and Coffee counties during the last week of September and continued into October.\"}\n",
    "event1 = {\"context\":\"Scattered thunderstorms developed ahead of a cold front on the morning of the 14th, producing isolated wind damage. Throughout the afternoon and evening of the 13th and all day on the 14th, strong south gradient winds associated with a strong low pressure system gusted up to 50 mph at times. The low pressure center tracked northeast across the Plains to the Great Lakes region. Peak gradient wind gusts included 56 mph at the Cape Girardeau airport, 53 mph at the Perryville airport, 48 mph at the Poplar Bluff airport, and 47 mph at the Sikeston airport.\"}\n",
    "\n",
    "\n",
    "def NLP_NER_OUTPUT(event):\n",
    "    url = 'https://0rwjoafjn8.execute-api.us-east-1.amazonaws.com/dev/qa'\n",
    "    response = requests.post(url,json=event1)\n",
    "    output = response.content.decode()\n",
    "    an = json.loads(output)\n",
    "    a = an['answer']\n",
    "    print(a)\n",
    "    b = str(a)\n",
    "    Final_dict = ''\n",
    "    list_b = list(b)\n",
    "    for i in range(1,len(list_b)-1):\n",
    "        Final_dict = Final_dict +list_b[i]\n",
    "\n",
    "    es = ast.literal_eval(Final_dict)\n",
    "    return es\n",
    "\n",
    "output_dict = NLP_NER_OUTPUT(event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f9e6bc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NLP_SUMMAARIZATION_OUTPUT(event):\n",
    "    url = 'https://yto0kae7xb.execute-api.us-east-1.amazonaws.com/dev/qa'\n",
    "    response = requests.post(url,json=event1)\n",
    "    output = response.content.decode()\n",
    "    an = json.loads(output)\n",
    "    a = an['answer']\n",
    "    return a\n",
    "\n",
    "output_dict = NLP_SUMMAARIZATION_OUTPUT(event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5b2425a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'entity': 'B-LOC',\n",
       "  'score': 0.9885201,\n",
       "  'index': 69,\n",
       "  'word': 'Plains',\n",
       "  'start': 346,\n",
       "  'end': 352},\n",
       " {'entity': 'B-LOC',\n",
       "  'score': 0.9972579,\n",
       "  'index': 72,\n",
       "  'word': 'Great',\n",
       "  'start': 360,\n",
       "  'end': 365},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.9981468,\n",
       "  'index': 73,\n",
       "  'word': 'Lakes',\n",
       "  'start': 366,\n",
       "  'end': 371},\n",
       " {'entity': 'B-LOC',\n",
       "  'score': 0.9952852,\n",
       "  'index': 87,\n",
       "  'word': 'Cape',\n",
       "  'start': 428,\n",
       "  'end': 432},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.99629253,\n",
       "  'index': 88,\n",
       "  'word': 'G',\n",
       "  'start': 433,\n",
       "  'end': 434},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.9961098,\n",
       "  'index': 89,\n",
       "  'word': '##ira',\n",
       "  'start': 434,\n",
       "  'end': 437},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.9663395,\n",
       "  'index': 90,\n",
       "  'word': '##rde',\n",
       "  'start': 437,\n",
       "  'end': 440},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.9272877,\n",
       "  'index': 91,\n",
       "  'word': '##au',\n",
       "  'start': 440,\n",
       "  'end': 442},\n",
       " {'entity': 'B-LOC',\n",
       "  'score': 0.9960742,\n",
       "  'index': 98,\n",
       "  'word': 'Perry',\n",
       "  'start': 466,\n",
       "  'end': 471},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.9977574,\n",
       "  'index': 99,\n",
       "  'word': '##ville',\n",
       "  'start': 471,\n",
       "  'end': 476},\n",
       " {'entity': 'B-LOC',\n",
       "  'score': 0.9915392,\n",
       "  'index': 106,\n",
       "  'word': 'Pop',\n",
       "  'start': 500,\n",
       "  'end': 503},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.94390976,\n",
       "  'index': 107,\n",
       "  'word': '##lar',\n",
       "  'start': 503,\n",
       "  'end': 506},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.99718875,\n",
       "  'index': 108,\n",
       "  'word': 'Bluff',\n",
       "  'start': 507,\n",
       "  'end': 512},\n",
       " {'entity': 'B-LOC',\n",
       "  'score': 0.9923455,\n",
       "  'index': 116,\n",
       "  'word': 'Si',\n",
       "  'start': 540,\n",
       "  'end': 542},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.9254696,\n",
       "  'index': 117,\n",
       "  'word': '##kes',\n",
       "  'start': 542,\n",
       "  'end': 545},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.7249604,\n",
       "  'index': 118,\n",
       "  'word': '##ton',\n",
       "  'start': 545,\n",
       "  'end': 548})"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0943d1cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31;40mB-LOC\u001b[0;31;40m \u001b[1;37;40mBoston\u001b[0;37;40m \n",
      "\n",
      "\u001b[1;31;40mI-LOC\u001b[0;31;40m \u001b[1;37;40mAmerica\u001b[0;37;40m \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(output_dict)):\n",
    "    if output_dict[i]['score'] > 0.90:\n",
    "        print(\"\\033[1;31;40m\"+output_dict[i]['entity']+\"\\033[0;31;40m\",\"\\033[1;37;40m\"+output_dict[i]['word']+\"\\033[0;37;40m \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93f3b8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "event = {\"context\":\"Hi My name is Petet\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea593c66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[{'entity': 'B-LOC', 'score': 0.9960104, 'index': 9, 'word': 'Geneva', 'start': 51, 'end': 57}, {'entity': 'B-LOC', 'score': 0.99175394, 'index': 11, 'word': 'Coffee', 'start': 62, 'end': 68}]\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "an['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "73c994ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"answer\": \"[{\\'entity\\': \\'B-LOC\\', \\'score\\': 0.9960104, \\'index\\': 9, \\'word\\': \\'Geneva\\', \\'start\\': 51, \\'end\\': 57}, {\\'entity\\': \\'B-LOC\\', \\'score\\': 0.99175394, \\'index\\': 11, \\'word\\': \\'Coffee\\', \\'start\\': 62, \\'end\\': 68}]\"}'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = response.content.decode()\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "7d639226",
   "metadata": {},
   "outputs": [],
   "source": [
    "an = json.loads(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c4af9ee3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': \"[{'entity': 'B-LOC', 'score': 0.9960104, 'index': 9, 'word': 'Geneva', 'start': 51, 'end': 57}, {'entity': 'B-LOC', 'score': 0.99175394, 'index': 11, 'word': 'Coffee', 'start': 62, 'end': 68}]\"}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "an"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc419ecf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "an['answer'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d13b0802",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[{'entity': 'B-PER', 'score': 0.95498997, 'index': 5, 'word': 'R', 'start': 14, 'end': 15}, {'entity': 'B-PER', 'score': 0.40459135, 'index': 6, 'word': '##iya', 'start': 15, 'end': 18}, {'entity': 'B-LOC', 'score': 0.73876435, 'index': 11, 'word': 'New', 'start': 33, 'end': 36}, {'entity': 'I-LOC', 'score': 0.7159286, 'index': 12, 'word': 'yo', 'start': 37, 'end': 39}]\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "an['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "537edb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = an['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "68620eb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[{'entity': 'B-LOC', 'score': 0.9960104, 'index': 9, 'word': 'Geneva', 'start': 51, 'end': 57}, {'entity': 'B-LOC', 'score': 0.99175394, 'index': 11, 'word': 'Coffee', 'start': 62, 'end': 68}]\""
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b9a64508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original list : [{'entity': 'B-LOC', 'score': 0.9960104, 'index': 9, 'word': 'Geneva', 'start': 51, 'end': 57}, {'entity': 'B-LOC', 'score': 0.99175394, 'index': 11, 'word': 'Coffee', 'start': 62, 'end': 68}]\n"
     ]
    }
   ],
   "source": [
    "print(\"The original list : \" + str(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "9a4f8fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = str(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "909d1dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = ''\n",
    "list_b = list(b)\n",
    "for i in range(1,len(list_b)-1):\n",
    "    c = c+list_b[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "ef8e04bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\'entity\\': \\'B-LOC\\', \\'score\\': 0.9960104, \\'index\\': 9, \\'word\\': \\'Geneva\\', \\'start\\': 51, \\'end\\': 57}, {\\'entity\\': \\'B-LOC\\', \\'score\\': 0.99175394, \\'index\\': 11, \\'word\\': \\'Coffee\\', \\'start\\': 62, \\'end\\': 68}\\'answer\\': \"[{\\'entity\\': \\'B-LOC\\', \\'score\\': 0.9960104, \\'index\\': 9, \\'word\\': \\'Geneva\\', \\'start\\': 51, \\'end\\': 57}, {\\'entity\\': \\'B-LOC\\', \\'score\\': 0.99175394, \\'index\\': 11, \\'word\\': \\'Coffee\\', \\'start\\': 62, \\'end\\': 68}]\"'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ebe858c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yo'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es[3]['word']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4a13d453",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "dictionary update sequence element #0 has length 1; 2 is required",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [75]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m()\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sub \u001b[38;5;129;01min\u001b[39;00m b:\n\u001b[0;32m----> 4\u001b[0m     \u001b[43mres\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msub\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe extracted value : \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(res[K]))\n",
      "\u001b[0;31mValueError\u001b[0m: dictionary update sequence element #0 has length 1; 2 is required"
     ]
    }
   ],
   "source": [
    "K = 'word'\n",
    "res = dict()\n",
    "for sub in b:\n",
    "    res.update(sub)\n",
    "print(\"The extracted value : \" + str(res[K]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c2bfd506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"[{'entity':\", \"'B-PER',\", \"'score':\", '0.95498997,', \"'index':\", '5,', \"'word':\", \"'R',\", \"'start':\", '14,', \"'end':\", '15},', \"{'entity':\", \"'B-PER',\", \"'score':\", '0.40459135,', \"'index':\", '6,', \"'word':\", \"'##iya',\", \"'start':\", '15,', \"'end':\", '18},', \"{'entity':\", \"'B-LOC',\", \"'score':\", '0.73876435,', \"'index':\", '11,', \"'word':\", \"'New',\", \"'start':\", '33,', \"'end':\", '36},', \"{'entity':\", \"'I-LOC',\", \"'score':\", '0.7159286,', \"'index':\", '12,', \"'word':\", \"'yo',\", \"'start':\", '37,', \"'end':\", '39}]']\n"
     ]
    }
   ],
   "source": [
    "def Convert(string):\n",
    "    li = list(string.split(\" \"))\n",
    "    return li\n",
    "  \n",
    "# Driver code    \n",
    "str1 = \"Geeks for Geeks\"\n",
    "print(Convert(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7adf743c",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [43]\u001b[0m, in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m resp \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(json_str)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m#print the resp\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#print (resp)\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m#extract an element in the response\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28mprint\u001b[39m (\u001b[43ma\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mword\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n",
      "\u001b[0;31mTypeError\u001b[0m: string indices must be integers"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import sys\n",
    "\n",
    "#load the data into an element\n",
    "data = a\n",
    "\n",
    "#dumps the json object into an element\n",
    "json_str = json.dumps(data)\n",
    "\n",
    "#load the json to a string\n",
    "resp = json.loads(json_str)\n",
    "\n",
    "#print the resp\n",
    "#print (resp)\n",
    "\n",
    "#extract an element in the response\n",
    "print (a['word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6cf125c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from re import sub\n",
    "\n",
    "def camel_case(s):\n",
    "  s = sub(r\"(_|-)+\", \" \", s).title()\n",
    "  return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6abfc262",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Scattered Thunderstorms Developed Ahead Of A Cold Front On The Morning Of The 14Th, Producing Isolated Wind Damage. Throughout The Afternoon And Evening Of The 13Th And All Day On The 14Th, Strong South Gradient Winds Associated With A Strong Low Pressure System Gusted Up To 50 Mph At Times. The Low Pressure Center Tracked Northeast Across The Plains To The Great Lakes Region. Peak Gradient Wind Gusts Included 56 Mph At The Cape Girardeau Airport, 53 Mph At The Perryville Airport, 48 Mph At The Poplar Bluff Airport, And 47 Mph At The Sikeston Airport.'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "camel_case(\"Scattered thunderstorms developed ahead of a cold front on the morning of the 14th, producing isolated wind damage. Throughout the afternoon and evening of the 13th and all day on the 14th, strong south gradient winds associated with a strong low pressure system gusted up to 50 mph at times. The low pressure center tracked northeast across the Plains to the Great Lakes region. Peak gradient wind gusts included 56 mph at the Cape Girardeau airport, 53 mph at the Perryville airport, 48 mph at the Poplar Bluff airport, and 47 mph at the Sikeston airport.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483d47b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 ('.venv': venv)",
   "language": "python",
   "name": "python380jvsc74a57bd0b4268bdeb8b3cd6119633cfedec5f8ccd1d344ae93aca82ac0bac4da2ecd2271"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
